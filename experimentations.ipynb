{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experimentations.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Bmghj3NjJ_m9",
        "yazAlJwRJ_nH",
        "GcI_gGDgJ_oW",
        "GJvpLDplFu9I",
        "fuQnWhXAO3ol",
        "NYnDVLp4F290",
        "oqdqp-XVJ_pY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YpQ-2ZhJ_mo"
      },
      "source": [
        "This notebook reproduces the evaluation and comparison of various learning approaches across 10 experiments.\n",
        "\n",
        "**Approaches**\n",
        "\n",
        "The approaches considered are listed below.\n",
        "\n",
        "Activity discovery\n",
        "- baseline activities\n",
        "- relational activities, based on instances ground truth (to assess potential)\n",
        "- relational activities, based on instances supervized baseline predictions\n",
        "- iterative relational approach  based on frozen instances predictions from supervized baseline\n",
        "\n",
        "Instances discovery\n",
        "- baseline instances (supervized)\n",
        "- baseline instances (unsupervized)\n",
        "- relational instances, based on activity ground truth (to assess potential)\n",
        "- relational instances, based on activity baseline predictions \n",
        "- iterative collaborative approach\n",
        "\n",
        "**Process**\n",
        "\n",
        "For each experience, the experimentation process:\n",
        "\n",
        "1.   generates dedicated train and test datasets\n",
        "2.   builds the corresponding classifiers\n",
        "3.   tests and evaluates them using ground truth labels\n",
        "\n",
        "Results are then averaged accross all experiences. Considering the long training time on Google Colab®, seeds are increased and set for each experience, in case the notebook disconnects. By doing so, one can resume training at the exact same point it was before the disconnection during an experience.\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- Original emails first to be need cleansed and formatted into a dataframe using the notebook 0_Camel_Email_Dataset_Extraction.ipynb. This dataframe then needs to be pre-processed using 1_Dataset_preprocessing.ipynb.\n",
        "\n",
        "- This notebook has been designed to be run on Google Colab®, using input and output files locations on Google Drive®. Therefore, the input CSV of preprocessed emails must be first uploaded to Google Drive® in order for this notebook to access it.\n",
        "\n",
        "- The function.py helper file must also be uploaded to Google Drive® in order for this notebook to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kya5KUV52QwE"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQzlShLS6HoE"
      },
      "source": [
        "Experience parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3mZEOZVJ_mu"
      },
      "source": [
        "nb_xp = 10\n",
        "nb_collaborative_iter = 5\n",
        "act_threshold = 0.95\n",
        "\n",
        "# Parameters if resuming the experimentation process\n",
        "resume = False # Set to True if resuming the experimentation process after a notebook disconnection between experiences\n",
        "nb_xp_total = 10 # Used to average the aggregated results after resuming\n",
        "seed = 42 # If resuming the experimentation process, input the corresponding incrementeed seed value at the resuming iteration\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW2vQG326VvT"
      },
      "source": [
        "Input and output files paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UiR-2tv6UsL"
      },
      "source": [
        "# Input files paths (Google Drive)\n",
        "email_embedded_path_csv = '/content/drive/MyDrive/input/path/to/uploaded/preprocessed/camel_emails_emb_s2v.csv'\n",
        "\n",
        "# Output files paths (Google Drive)\n",
        "results_sup_save_path = '/content/drive/MyDrive/output/path/for/results_supervized_approaches.pickle'\n",
        "results_unsup_save_path = '/content/drive/MyDrive/output/path/for/results_unsupervized_approaches.pickle'\n",
        "user_metrics_save_path = '/content/drive/MyDrive/output/path/for/results_user_queries.pickle'\n",
        "best_preds_path = '/content/drive/MyDrive/output/path/for/emails/predictions.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JmV38dQ6ZTV"
      },
      "source": [
        "Connect to Google Drive® to access uploaded files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWGqBvsK0mu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XthMWf0V6h-v"
      },
      "source": [
        "Copy the uploaded helper functions.py to the current environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_0-cd7ELaHI"
      },
      "source": [
        "!cp /content/drive/MyDrive/input/path/uploaded/functions.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc6CkYNpJ_mz"
      },
      "source": [
        "\n",
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4faYdfP4LI8D"
      },
      "source": [
        "!pip install spacy --upgrade\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4rf1Bk1J_m3"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import math\n",
        "import spacy\n",
        "import random\n",
        "import csv\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDW1T1ZGLN9H"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V22M4d-MMVw4"
      },
      "source": [
        "from functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRsP6d_dJ_m4"
      },
      "source": [
        "random.seed(seed)\n",
        "np.random.seed(seed) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmghj3NjJ_m9"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPQC3cQvJ_m_"
      },
      "source": [
        "emails_relational = pd.read_csv(email_embedded_path_csv, quoting=csv.QUOTE_ALL).sort_values('Email_ID')\n",
        "# Remove single-step instances and single-instance activities\n",
        "emails_relational = filter_emails_for_split(emails_relational, [1, 5, 6, 7, 9, 12, 14, 16, 17, 19, 20, 24, 26, 27, 28, 29, 32, 33, 34, 38, 39, 43, 44, 46, 47, 51, 54, 55, 56, 58, 59, 60, 64, 65])\n",
        "emails_relational, label_encoder, onehot_encoder, ohe_domain = preprocess(emails_relational, nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75PdBSLaJ_nF"
      },
      "source": [
        "# Experiences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yazAlJwRJ_nH"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToPntd1J_nI"
      },
      "source": [
        "def test_approaches(\n",
        "    threshold_act, rel_acts_output_classes,\n",
        "    train, test, n_iter,\n",
        "    baseline_inst_sup, baseline_inst_unsup, \n",
        "    baseline_act_sup, \n",
        "    clf_rel_inst, \n",
        "    clf_rel_act, \n",
        "    onehot_encoder, label_encoder, ohe_domain, nlp,\n",
        "    approaches=[\n",
        "        'rel_with_gt_inst', 'rel_with_gt_act', \n",
        "        'rel_with_baseline_inst', 'rel_with_baseline_act',\n",
        "        'collab_inst_and_act',\n",
        "        'collab_threshold_inst_and_act',\n",
        "        'iterative_with_frozen_baseline_inst_act',\n",
        "        'iterative_threshold_with_frozen_baseline_inst_act'\n",
        "    ],\n",
        "    baselines=True\n",
        "):\n",
        "    \n",
        "    \n",
        "    X = test.copy(deep=True)  \n",
        "    generations = nb_generations(train) + 1\n",
        "    \n",
        "    user_metrics = {\n",
        "        'ground_truth_inst_metrics': {\n",
        "            'avg_length_inst': get_avg_max_length_inst(X, 'Trace_ID')[0],\n",
        "            'max_length_inst': get_avg_max_length_inst(X, 'Trace_ID')[1],\n",
        "            'avg_steps_inst': get_avg_max_steps_inst(X, 'Trace_ID')[0],\n",
        "            'max_steps_inst': get_avg_max_steps_inst(X, 'Trace_ID')[1],\n",
        "            'avg_nb_users_inst': get_avg_max_nb_users_inst(X, 'Trace_ID')[0],\n",
        "            'max_nb_users_inst': get_avg_max_nb_users_inst(X, 'Trace_ID')[1]\n",
        "        },\n",
        "        'ground_truth_acts_metrics': {\n",
        "            'avg_length_act': get_avg_max_length_act(X, 'Action', label_encoder)[0][('act_length', 'mean')],\n",
        "            'max_length_act': get_avg_max_length_act(X, 'Action', label_encoder)[1][('act_length', 'max')],\n",
        "            'avg_nb_users_act': get_avg_max_nb_users_act(X, 'Action', label_encoder)[0][('nb_users', 'mean')],\n",
        "            'max_nb_users_act': get_avg_max_nb_users_act(X, 'Action', label_encoder)[1][('nb_users', 'max')]\n",
        "        },\n",
        "        'relational_inst_metrics' : [],\n",
        "        'relational_acts_metrics_from_sup_inst' : [],\n",
        "        'relational_acts_metrics_from_unsup_inst' : [],\n",
        "        'relational_inst_metrics_collab': [],\n",
        "        'relational_inst_metrics_collab_threshold': [],\n",
        "        'relational_acts_metrics_from_sup_inst_collab' : [],\n",
        "        'relational_acts_metrics_from_sup_inst_frozen' : [],\n",
        "        'relational_acts_metrics_from_sup_inst_collab_threshold': [],\n",
        "        'relational_acts_metrics_from_sup_inst_frozen_threshold': []\n",
        "    }\n",
        "    \n",
        "    results_sup = []\n",
        "    results_unsup = []\n",
        "    \n",
        "    ###\n",
        "    ### Apply and test baselines\n",
        "    ###\n",
        "    \n",
        "    if baselines:\n",
        "    \n",
        "        ## Instances \n",
        "\n",
        "        # Supervized\n",
        "        X_pairs, X_insts_sup, y_instances = get_X_y_instances(X, nlp)\n",
        "        pred_instances_sup = baseline_inst_sup.predict(X_insts_sup)\n",
        "        for pair, pred in zip(X_pairs, pred_instances_sup):\n",
        "            label_pairs_of_rows_from_ground_truth_instances(pair)\n",
        "            pair['same_instance_pred_sup'] = pred\n",
        "        precision, recall, f_score = evaluate_instances_discovery(X_pairs, 'same_instance', 'same_instance_pred_sup')\n",
        "        results_baseline_inst_sup = {\n",
        "            'baseline_instances_sup':{\n",
        "                'precision':precision,\n",
        "                'recall':recall,\n",
        "                'f1':f_score\n",
        "            }\n",
        "        }\n",
        "        results_sup.append(results_baseline_inst_sup)\n",
        "        X_baseline = get_trace_from_insts_preds(X, X_pairs, 'same_instance_pred_sup', 'Trace_ID_pred_sup')\n",
        "        X_pairs_sup = get_pairs_of_rows(X_baseline)\n",
        "        X = X.merge(X_baseline[['Email_ID', 'Trace_ID_pred_sup']], on='Email_ID', how='left')\n",
        "\n",
        "        # Unsupervized\n",
        "        pred_instances_unsup = baseline_inst_unsup(X)\n",
        "        X['Trace_ID_pred_unsup'] = pred_instances_unsup\n",
        "        X_pairs_unsup = get_pairs_of_rows(X)\n",
        "        for pair_unsup in X_pairs_unsup:\n",
        "            label_pairs_of_rows_from_ground_truth_instances(pair_unsup)\n",
        "            pair_unsup_traces = pair_unsup['Trace_ID_pred_unsup'].drop_duplicates()\n",
        "            if pair_unsup_traces.shape[0] == 1 :\n",
        "                pair_unsup['same_instance_pred_unsup'] = 1 \n",
        "            else:\n",
        "                pair_unsup['same_instance_pred_unsup'] = 0\n",
        "        precision, recall, f_score = evaluate_instances_discovery(X_pairs_unsup, 'same_instance', 'same_instance_pred_unsup')\n",
        "        results_baseline_inst_unsup = {\n",
        "            'baseline_instances_unsup':{\n",
        "                'precision':precision,\n",
        "                'recall':recall,\n",
        "                'f1':f_score\n",
        "            }\n",
        "        }\n",
        "        results_unsup.append(results_baseline_inst_unsup)\n",
        "        \n",
        "        user_metrics['baseline_inst_sup_metrics'] = {\n",
        "            'avg_length_inst': get_avg_max_length_inst(X_baseline, 'Trace_ID_pred_sup')[0],\n",
        "            'max_length_inst': get_avg_max_length_inst(X_baseline, 'Trace_ID_pred_sup')[1],\n",
        "            'avg_steps_inst': get_avg_max_steps_inst(X_baseline, 'Trace_ID_pred_sup')[0],\n",
        "            'max_steps_inst': get_avg_max_steps_inst(X_baseline, 'Trace_ID_pred_sup')[1],\n",
        "            'avg_nb_users_inst': get_avg_max_nb_users_inst(X_baseline, 'Trace_ID_pred_sup')[0],\n",
        "            'max_nb_users_inst': get_avg_max_nb_users_inst(X_baseline, 'Trace_ID_pred_sup')[1]\n",
        "        }\n",
        "        user_metrics['baseline_inst_unsup_metrics'] = {\n",
        "            'avg_length_inst': get_avg_max_length_inst(X, 'Trace_ID_pred_unsup')[0],\n",
        "            'max_length_inst': get_avg_max_length_inst(X, 'Trace_ID_pred_unsup')[1],\n",
        "            'avg_steps_inst': get_avg_max_steps_inst(X, 'Trace_ID_pred_unsup')[0],\n",
        "            'max_steps_inst': get_avg_max_steps_inst(X, 'Trace_ID_pred_unsup')[1],\n",
        "            'avg_nb_users_inst': get_avg_max_nb_users_inst(X, 'Trace_ID_pred_unsup')[0],\n",
        "            'max_nb_users_inst': get_avg_max_nb_users_inst(X, 'Trace_ID_pred_unsup')[1]\n",
        "        }\n",
        "\n",
        "\n",
        "        ## Activities\n",
        "\n",
        "        X_acts_sup, y_acts = get_X_y_activities(X, label_encoder, ohe_domain)\n",
        "        pred_activities_sup = baseline_act_sup.predict(X_acts_sup)\n",
        "        score_baseline_act_sup = precision_recall_fscore_support(y_acts, pred_activities_sup, average='micro')\n",
        "        X['act_pred_sup'] = label_encoder.inverse_transform(pred_activities_sup)\n",
        "\n",
        "        results_baseline_act_sup = {\n",
        "            'baseline_activities_sup':{\n",
        "                'unpredicted_labels': set(y_acts) - set(pred_activities_sup),\n",
        "                'precision':score_baseline_act_sup[0],\n",
        "                'recall':score_baseline_act_sup[1],\n",
        "                # 'f1':score_baseline_act_sup[2]\n",
        "                'f1':f1_score(y_acts, pred_activities_sup, average=None)\n",
        "\n",
        "            }\n",
        "        }\n",
        "        results_sup.append(results_baseline_act_sup)\n",
        "\n",
        "        user_metrics['baseline_act_sup_metrics'] = {\n",
        "            'avg_length_act': get_avg_max_length_act(X, 'act_pred_sup', label_encoder)[0][('act_length', 'mean')],\n",
        "            'max_length_act': get_avg_max_length_act(X, 'act_pred_sup', label_encoder)[1][('act_length', 'max')],\n",
        "            'avg_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_sup', label_encoder)[0][('nb_users', 'mean')],\n",
        "            'max_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_sup', label_encoder)[1][('nb_users', 'max')]\n",
        "        }\n",
        "    \n",
        "    \n",
        "    ###\n",
        "    ### Relational with ground truth\n",
        "    ###\n",
        "    \n",
        "    \n",
        "    if 'rel_with_gt_inst' in approaches:\n",
        "    \n",
        "        ## Instances\n",
        "\n",
        "        X_pairs, X_insts_rel_sup, y_instances = get_X_y_instances_rel(X, 'Action', nlp, onehot_encoder)\n",
        "        pred_instances_rel_gt_sup = clf_rel_inst.predict(X_insts_rel_sup)\n",
        "        pred_instances_rel_gt_sup_list = []\n",
        "        for pair, pred in zip(X_pairs, pred_instances_rel_gt_sup):\n",
        "            label_pairs_of_rows_from_ground_truth_instances(pair)\n",
        "            pair['same_instance_pred_rel_gt_sup'] = pred\n",
        "        precision, recall, f_score = evaluate_instances_discovery(X_pairs, 'same_instance', 'same_instance_pred_rel_gt_sup')\n",
        "        X_rel_gt = get_trace_from_insts_preds(X, X_pairs, 'same_instance_pred_rel_gt_sup', 'Trace_ID_pred_rel_gt_sup')\n",
        "        X_pairs_rel_gt = get_pairs_of_rows(X_rel_gt)\n",
        "\n",
        "        results_rel_gt_inst_sup = {\n",
        "            'rel_gt_instances_sup':{\n",
        "                'precision':precision,\n",
        "                'recall':recall,\n",
        "                'f1':f_score\n",
        "            }\n",
        "        }\n",
        "        results_sup.append(results_rel_gt_inst_sup)\n",
        "        \n",
        "        user_metrics['rel_gt_inst_sup_metrics'] = {\n",
        "            'avg_length_inst': get_avg_max_length_inst(X_rel_gt, 'Trace_ID_pred_rel_gt_sup')[0],\n",
        "            'max_length_inst': get_avg_max_length_inst(X_rel_gt, 'Trace_ID_pred_rel_gt_sup')[1],\n",
        "            'avg_steps_inst': get_avg_max_steps_inst(X_rel_gt, 'Trace_ID_pred_rel_gt_sup')[0],\n",
        "            'max_steps_inst': get_avg_max_steps_inst(X_rel_gt, 'Trace_ID_pred_rel_gt_sup')[1],\n",
        "            'avg_nb_users_inst': get_avg_max_nb_users_inst(X_rel_gt, 'Trace_ID_pred_rel_gt_sup')[0],\n",
        "            'max_nb_users_inst': get_avg_max_nb_users_inst(X_rel_gt, 'Trace_ID_pred_rel_gt_sup')[1]\n",
        "        }\n",
        "    \n",
        "    if 'rel_with_gt_act' in approaches:\n",
        "    \n",
        "        ## Activities\n",
        "\n",
        "        X_acts_rel_sup, y_acts = get_X_y_activities_rel(X, 'Trace_ID', 'Action', label_encoder, onehot_encoder, ohe_domain, generations)\n",
        "        pred_activities_rel_sup = clf_rel_act.predict(X_acts_rel_sup)\n",
        "        score_rel_act_sup = precision_recall_fscore_support(y_acts, pred_activities_rel_sup, average='micro')\n",
        "        X['act_pred_rel_gt_sup'] = label_encoder.inverse_transform(pred_activities_rel_sup)\n",
        "\n",
        "        results_rel_gt_act_sup = {\n",
        "            'rel_gt_activities_sup':{\n",
        "                'unpredicted_labels': set(y_acts) - set(pred_activities_rel_sup),\n",
        "                'precision':score_rel_act_sup[0],\n",
        "                'recall':score_rel_act_sup[1],\n",
        "                'f1':f1_score(y_acts, pred_activities_rel_sup, average=None)\n",
        "\n",
        "            }\n",
        "        }\n",
        "        results_sup.append(results_rel_gt_act_sup)\n",
        "        \n",
        "        user_metrics['rel_gt_act_sup_metrics'] = {\n",
        "            'avg_length_act': get_avg_max_length_act(X, 'act_pred_rel_gt_sup', label_encoder)[0][('act_length', 'mean')],\n",
        "            'max_length_act': get_avg_max_length_act(X, 'act_pred_rel_gt_sup', label_encoder)[1][('act_length', 'max')],\n",
        "            'avg_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_rel_gt_sup', label_encoder)[0][('nb_users', 'mean')],\n",
        "            'max_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_rel_gt_sup', label_encoder)[1][('nb_users', 'max')]\n",
        "        }\n",
        "    \n",
        "    \n",
        "    ###\n",
        "    ### Relational with baselines predictions\n",
        "    ###\n",
        "    \n",
        "    \n",
        "    rel_baseline_results = {}\n",
        "\n",
        "    if 'rel_with_baseline_inst' in approaches:\n",
        "\n",
        "        ## Instances with activity baseline preds\n",
        "\n",
        "        X_pairs, X_insts_rel_sup, y_instances = get_X_y_instances_rel(X, 'act_pred_sup', nlp, onehot_encoder)\n",
        "        pred_instances_rel_sup = clf_rel_inst.predict(X_insts_rel_sup)\n",
        "        pred_instances_rel_sup_list = []\n",
        "        for pair, pred in zip(X_pairs, pred_instances_rel_sup):\n",
        "            label_pairs_of_rows_from_ground_truth_instances(pair)\n",
        "            pair['same_instance_pred_rel_sup'] = pred\n",
        "        precision, recall, f_score = evaluate_instances_discovery(X_pairs, 'same_instance', 'same_instance_pred_rel_sup')\n",
        "        X_rel_sup = get_trace_from_insts_preds(X, X_pairs, 'same_instance_pred_rel_sup', 'Trace_ID_pred_rel_sup')\n",
        "        X_pairs_rel_sup = get_pairs_of_rows(X_rel_sup)\n",
        "        \n",
        "        rel_baseline_results['rel_instances'] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f_score\n",
        "        }\n",
        "        user_metrics['relational_inst_metrics'] ={\n",
        "            'avg_length_inst': get_avg_max_length_inst(X_rel_sup, 'Trace_ID_pred_rel_sup')[0],\n",
        "            'max_length_inst': get_avg_max_length_inst(X_rel_sup, 'Trace_ID_pred_rel_sup')[1],\n",
        "            'avg_steps_inst': get_avg_max_steps_inst(X_rel_sup, 'Trace_ID_pred_rel_sup')[0],\n",
        "            'max_steps_inst': get_avg_max_steps_inst(X_rel_sup, 'Trace_ID_pred_rel_sup')[1],\n",
        "            'avg_nb_users_inst': get_avg_max_nb_users_inst(X_rel_sup, 'Trace_ID_pred_rel_sup')[0],\n",
        "            'max_nb_users_inst': get_avg_max_nb_users_inst(X_rel_sup, 'Trace_ID_pred_rel_sup')[1]\n",
        "        }\n",
        "    \n",
        "    if 'rel_with_baseline_act' in approaches:\n",
        "\n",
        "        ## Activity with instances baseline preds\n",
        "\n",
        "        # With instances supervized baseline preds\n",
        "        X_acts_rel_sup, y_acts = get_X_y_activities_rel(X, 'Trace_ID_pred_sup', 'act_pred_sup', label_encoder, onehot_encoder, ohe_domain, generations)\n",
        "        pred_activities_rel_sup = clf_rel_act.predict(X_acts_rel_sup)\n",
        "        score_rel_act_sup = precision_recall_fscore_support(y_acts, pred_activities_rel_sup, average='micro')\n",
        "        X['act_pred_sup_from_sup_inst'] = label_encoder.inverse_transform(pred_activities_rel_sup)\n",
        "\n",
        "        rel_baseline_results_sup = copy.deepcopy(rel_baseline_results)\n",
        "        rel_baseline_results_sup['rel_activities_from_sup_inst'] = {\n",
        "            'unpredicted_labels': set(y_acts) - set(pred_activities_rel_sup),\n",
        "            'precision':score_rel_act_sup[0],\n",
        "            'recall':score_rel_act_sup[1],\n",
        "            'f1':f1_score(y_acts, pred_activities_rel_sup, average=None)\n",
        "\n",
        "        }\n",
        "        user_metrics['relational_acts_metrics_from_sup_inst'] = {\n",
        "            'avg_length_act': get_avg_max_length_act(X, 'act_pred_sup_from_sup_inst', label_encoder)[0][('act_length', 'mean')],\n",
        "            'max_length_act': get_avg_max_length_act(X, 'act_pred_sup_from_sup_inst', label_encoder)[1][('act_length', 'max')],\n",
        "            'avg_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_sup_from_sup_inst', label_encoder)[0][('nb_users', 'mean')],\n",
        "            'max_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_sup_from_sup_inst', label_encoder)[1][('nb_users', 'max')]\n",
        "        }\n",
        "\n",
        "    results_sup.append(rel_baseline_results_sup)        \n",
        "        \n",
        "    ###    \n",
        "    ### Relational collaborative\n",
        "    ###\n",
        "    \n",
        "    \n",
        "    X_frozen = copy.deepcopy(X)\n",
        "    X_frozen_threshold = copy.deepcopy(X)\n",
        "    X_collab_threshold = copy.deepcopy(X)\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        \n",
        "        rel_collab_results = {}\n",
        "        rel_collab_threshold_results = {}\n",
        "        \n",
        "        if 'iterative_threshold_with_frozen_baseline_inst_act' in approaches:\n",
        "        \n",
        "            ## Activities with frozen instances from supervized baseline\n",
        "\n",
        "            X_frozen_acts_rel_sup, y_frozen_acts = get_X_y_activities_rel(X_frozen_threshold, 'Trace_ID_pred_sup', 'act_pred_sup', label_encoder, onehot_encoder, ohe_domain, generations)\n",
        "            pred_frozen_activities_rel_sup = clf_rel_act.predict_proba(X_frozen_acts_rel_sup)\n",
        "            X_frozen_threshold['act_pred_amax'] = np.argmax(pred_frozen_activities_rel_sup, 1)\n",
        "            X_frozen_threshold['act_pred_max'] = [np.max(x) for x in pred_frozen_activities_rel_sup]\n",
        "            X_frozen_threshold['act_pred_sup'] = np.where(X_frozen_threshold['act_pred_max'] > threshold_act, label_encoder.inverse_transform(rel_acts_output_classes[X_frozen_threshold['act_pred_amax']]), X_frozen_threshold['act_pred_sup'])\n",
        "            pred_frozen_activities_rel_sup_list = label_encoder.transform(X_frozen_threshold['act_pred_sup'])\n",
        "            score_frozen_rel_act_sup = precision_recall_fscore_support(y_frozen_acts, pred_frozen_activities_rel_sup_list, average='micro')\n",
        "\n",
        "            rel_frozen_threshold_results = {\n",
        "                'rel_activities_from_sup_inst_frozen_threshold': {\n",
        "                    'unpredicted_labels': set(),\n",
        "                    'precision':score_frozen_rel_act_sup[0],\n",
        "                    'recall':score_frozen_rel_act_sup[1],\n",
        "                    'f1':f1_score(y_frozen_acts, pred_frozen_activities_rel_sup_list, average=None)\n",
        "\n",
        "                }\n",
        "            }\n",
        "            results_sup.append(rel_frozen_threshold_results)\n",
        "            \n",
        "            user_metrics['relational_acts_metrics_from_sup_inst_frozen_threshold'].append(\n",
        "                {\n",
        "                    'avg_length_act': get_avg_max_length_act(X_frozen, 'act_pred_sup', label_encoder)[0][('act_length', 'mean')],\n",
        "                    'max_length_act': get_avg_max_length_act(X_frozen, 'act_pred_sup', label_encoder)[1][('act_length', 'max')],\n",
        "                    'avg_nb_users_act': get_avg_max_nb_users_act(X_frozen, 'act_pred_sup', label_encoder)[0][('nb_users', 'mean')],\n",
        "                    'max_nb_users_act': get_avg_max_nb_users_act(X_frozen, 'act_pred_sup', label_encoder)[1][('nb_users', 'max')]\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # For best predictions CSV output\n",
        "            if i == (n_iter-1):\n",
        "              print(\"Writing best preds to CSV.\")\n",
        "              df = X_frozen_threshold[['Email_ID', 'Trace_ID', 'Action', 'Trace_ID_pred_sup', 'act_pred_sup']]\n",
        "              df.to_csv(best_preds_path, index=False)            \n",
        "\n",
        "        if 'collab_inst_and_act' in approaches:\n",
        "            \n",
        "            ## Instances with activity rel preds\n",
        "\n",
        "            X_pairs, X_insts_collab, y_instances = get_X_y_instances_rel(X, 'act_pred_sup', nlp, onehot_encoder)\n",
        "            pred_instances_collab = clf_rel_inst.predict(X_insts_collab)\n",
        "            pred_instances_collab_list = []\n",
        "            for pair, pred in zip(X_pairs, pred_instances_collab):\n",
        "                label_pairs_of_rows_from_ground_truth_instances(pair)\n",
        "                pair['same_instance_pred_rel_sup'] = pred\n",
        "            X_collab = get_trace_from_insts_preds(X, X_pairs, 'same_instance_pred_rel_sup', 'Trace_ID_pred_sup')\n",
        "            X_pairs_collab = get_pairs_of_rows(X_collab)\n",
        "            precision, recall, f_score = evaluate_instances_discovery(X_pairs, 'same_instance', 'same_instance_pred_rel_sup')\n",
        "            rel_collab_results['rel_instances_collab'] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f_score\n",
        "            }\n",
        "            user_metrics['relational_inst_metrics_collab'].append(\n",
        "                {\n",
        "                    'avg_length_inst': get_avg_max_length_inst(X_collab, 'Trace_ID_pred_sup')[0],\n",
        "                    'max_length_inst': get_avg_max_length_inst(X_collab, 'Trace_ID_pred_sup')[1],\n",
        "                    'avg_steps_inst': get_avg_max_steps_inst(X_collab, 'Trace_ID_pred_sup')[0],\n",
        "                    'max_steps_inst': get_avg_max_steps_inst(X_collab, 'Trace_ID_pred_sup')[1],\n",
        "                    'avg_nb_users_inst': get_avg_max_nb_users_inst(X_collab, 'Trace_ID_pred_sup')[0],\n",
        "                    'max_nb_users_inst': get_avg_max_nb_users_inst(X_collab, 'Trace_ID_pred_sup')[1]\n",
        "                }\n",
        "            )\n",
        "\n",
        "            ## Activities with instances rel preds\n",
        "\n",
        "            # With instances supervized rel preds\n",
        "            X_acts_rel_sup, y_acts = get_X_y_activities_rel(X, 'Trace_ID_pred_sup', 'act_pred_sup', label_encoder, onehot_encoder, ohe_domain, generations)\n",
        "            pred_activities_rel_sup = clf_rel_act.predict(X_acts_rel_sup)\n",
        "            score_rel_act_sup = precision_recall_fscore_support(y_acts, pred_activities_rel_sup, average='micro')\n",
        "            X['act_pred_sup'] = label_encoder.inverse_transform(pred_activities_rel_sup)\n",
        "\n",
        "            rel_collab_results['rel_activities_from_sup_inst_collab'] = {\n",
        "                'unpredicted_labels': set(y_acts) - set(pred_activities_rel_sup),\n",
        "                'precision':score_rel_act_sup[0],\n",
        "                'recall':score_rel_act_sup[1],\n",
        "                'f1':f1_score(y_acts, pred_activities_rel_sup, average=None)\n",
        "\n",
        "            }\n",
        "            results_sup.append(rel_collab_results)\n",
        "            \n",
        "            user_metrics['relational_acts_metrics_from_sup_inst_collab'].append(\n",
        "                {\n",
        "                    'avg_length_act': get_avg_max_length_act(X, 'act_pred_sup', label_encoder)[0][('act_length', 'mean')],\n",
        "                    'max_length_act': get_avg_max_length_act(X, 'act_pred_sup', label_encoder)[1][('act_length', 'max')],\n",
        "                    'avg_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_sup', label_encoder)[0][('nb_users', 'mean')],\n",
        "                    'max_nb_users_act': get_avg_max_nb_users_act(X, 'act_pred_sup', label_encoder)[1][('nb_users', 'max')]\n",
        "                }\n",
        "            )\n",
        "        \n",
        "            # Update instances for next iteration\n",
        "            X['Trace_ID_pred_sup'] = X_collab['Trace_ID_pred_sup']\n",
        "        \n",
        "        # Decreasing treshold\n",
        "        threshold_act = threshold_act / 2\n",
        "\n",
        "    return results_sup, results_unsup, user_metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C25F6xqJ_oB"
      },
      "source": [
        "def experiences(n_xp, nb_collaborative_iter, act_threshold, onehot_encoder, label_encoder, ohe_domain, nlp, approaches=[\n",
        "    'rel_with_gt_inst', 'rel_with_gt_act',\n",
        "    'rel_with_baseline_inst', 'rel_with_baseline_act',\n",
        "    'collab_inst_and_act',\n",
        "    'collab_threshold_inst_and_act',\n",
        "    'iterative_with_frozen_baseline_inst_act',\n",
        "    'iterative_threshold_with_frozen_baseline_inst_act'\n",
        "], resume=False, seed=42):\n",
        "    results_sup = []\n",
        "    results_unsup = []\n",
        "    user_metrics = []\n",
        "\n",
        "    if resume:\n",
        "      with open(results_sup_save_path, 'rb') as f:\n",
        "        results_sup = pickle.load(f)\n",
        "      with open(results_unsup_save_path, 'rb') as f:\n",
        "        results_unsup = pickle.load(f)\n",
        "      with open(user_metrics_save_path, 'rb') as f:\n",
        "        user_metrics = pickle.load(f)\n",
        "\n",
        "    for i in range(n_xp):\n",
        "        print(\"=====================\")\n",
        "        print(\"START EXPERIENCE: \", str(i))\n",
        "\n",
        "        seed = seed + 1\n",
        "        print(\"Seed value: \", seed)\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "        train, test = split_train_test(emails_relational)\n",
        "        \n",
        "        supervized_baseline_instances_structured(train, test, nlp, seed=42, save_path=\"baseline_inst.pickle.dat\")\n",
        "        supervized_baseline_activities_structured(train, test, label_encoder, ohe_domain, seed=seed, save_path=\"baseline_act.pickle.dat\")\n",
        "        supervized_relational_instances_structured(train, test, nlp, onehot_encoder, seed=42, save_path=\"relational_inst.pickle.dat\")\n",
        "        supervized_relational_activities_structured(train, test, label_encoder, onehot_encoder, ohe_domain, seed=seed, save_path=\"relational_act.pickle.dat\")     \n",
        "            \n",
        "        clf_supervized_baseline_instances = pickle.load(open(\"baseline_inst.pickle.dat\", \"rb\"))\n",
        "        clf_supervized_baseline_activities = pickle.load(open(\"baseline_act.pickle.dat\", \"rb\"))\n",
        "        model_relational_instances = pickle.load(open(\"relational_inst.pickle.dat\", \"rb\"))\n",
        "        model_relational_activities = pickle.load(open(\"relational_act.pickle.dat\", \"rb\"))\n",
        "        rel_acts_output_classes = model_relational_activities.classes_\n",
        "        \n",
        "        res_sup, res_unsup, u_metrics = test_approaches(\n",
        "            act_threshold, rel_acts_output_classes,\n",
        "            train, test, nb_collaborative_iter,\n",
        "            clf_supervized_baseline_instances, unsupervized_baseline_instances,\n",
        "            clf_supervized_baseline_activities, \n",
        "            model_relational_instances, \n",
        "            model_relational_activities,\n",
        "            onehot_encoder, label_encoder, ohe_domain, nlp,\n",
        "            approaches, \n",
        "            baselines=True\n",
        "        )\n",
        "        \n",
        "        results_sup.append(res_sup)\n",
        "        results_unsup.append(res_unsup)\n",
        "        user_metrics.append(u_metrics)\n",
        "        \n",
        "        del train\n",
        "        del test\n",
        "\n",
        "        # Save results and seed betwween each iteration\n",
        "        with open(results_sup_save_path, 'wb') as f:\n",
        "          pickle.dump(results_sup, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(results_sup_save_path, 'rb') as f:\n",
        "            results_sup = pickle.load(f)\n",
        "        \n",
        "        with open(results_unsup_save_path, 'wb') as f:\n",
        "            pickle.dump(results_unsup, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(results_unsup_save_path, 'rb') as f:\n",
        "            results_unsup = pickle.load(f)\n",
        "\n",
        "        with open(user_metrics_save_path, 'wb') as f:\n",
        "            pickle.dump(user_metrics, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(user_metrics_save_path, 'rb') as f:\n",
        "            user_metrics = pickle.load(f)\n",
        "\n",
        "        print(\"END EXPERIENCE: \", str(i))\n",
        "        print(\"=====================\")\n",
        "        \n",
        "    return results_sup, results_unsup, user_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIJBv2aRJ_oO"
      },
      "source": [
        "## Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YD709W3J_oQ"
      },
      "source": [
        "We evaluate our various approaches and compare their results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxVN66P-J_oS"
      },
      "source": [
        "%%time\n",
        "results_sup, results_unsup, user_metrics = experiences(nb_xp, nb_collaborative_iter, act_threshold, onehot_encoder, label_encoder, ohe_domain, nlp, approaches=[\n",
        "    'rel_with_gt_inst', 'rel_with_gt_act',\n",
        "    'rel_with_baseline_inst', 'rel_with_baseline_act',\n",
        "    'collab_inst_and_act',\n",
        "    'iterative_threshold_with_frozen_baseline_inst_act'\n",
        "], resume=resume, seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcI_gGDgJ_oW"
      },
      "source": [
        "# Average results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvpLDplFu9I"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJwwYbrcJ_oX"
      },
      "source": [
        "def clean_nan_dict(d):\n",
        "    gt_inst_metrics = d.get('ground_truth_inst_metrics')\n",
        "    gt_act_metrics = d.get('ground_truth_acts_metrics')\n",
        "    baseline_inst_sup_metrics = d.get('baseline_inst_sup_metrics')\n",
        "    baseline_inst_unsup_metrics = d.get('baseline_inst_unsup_metrics')\n",
        "    baseline_act_sup_metrics = d.get('baseline_act_sup_metrics')\n",
        "    rel_baseline_inst_metrics = d.get('relational_inst_metrics')\n",
        "    rel_act_from_sup_frozen_metrics_threshold = d.get('relational_acts_metrics_from_sup_inst_frozen_threshold')\n",
        "    \n",
        "    for i, elt in enumerate(rel_act_from_sup_frozen_metrics_threshold):\n",
        "        for k,v in elt.items():\n",
        "            for v, metric in elt[k].items():\n",
        "                if math.isnan(metric):\n",
        "                    rel_act_from_sup_frozen_metrics_threshold[i][k][v] = 0\n",
        "                                  \n",
        "    for k,v in baseline_act_sup_metrics.items():\n",
        "        for v, metric in baseline_act_sup_metrics[k].items():\n",
        "            if math.isnan(metric):\n",
        "                baseline_act_sup_metrics[k][v] = 0\n",
        "            \n",
        "    for k, metrics in baseline_inst_sup_metrics.items():\n",
        "        if math.isnan(metric):\n",
        "            baseline_inst_sup_metrics[k] = 0\n",
        "                            \n",
        "    for k,v in gt_act_metrics.items():\n",
        "        for v, metric in gt_act_metrics[k].items():\n",
        "            if math.isnan(metric):\n",
        "                gt_act_metrics[k][v] = 0\n",
        "            \n",
        "    for k, metrics in gt_inst_metrics.items():\n",
        "        if math.isnan(metric):\n",
        "            gt_inst_metrics[k] = 0\n",
        "            \n",
        "    result = {\n",
        "        'ground_truth_inst_metrics': gt_inst_metrics,\n",
        "        'ground_truth_acts_metrics': gt_act_metrics,\n",
        "        'baseline_inst_sup_metrics': baseline_inst_sup_metrics,\n",
        "        'baseline_act_sup_metrics' : baseline_act_sup_metrics,\n",
        "    }\n",
        "    \n",
        "    return result\n",
        "                 \n",
        "def get_avg_results_sup(results, nb_xp, nb_collaborative_iter):\n",
        "    summed_results = {\n",
        "        'baseline_instances_sup': {\n",
        "            'f1': 0,\n",
        "            'precision': 0,\n",
        "            'recall': 0\n",
        "        },\n",
        "        'baseline_activities_sup': {\n",
        "            'f1': [],\n",
        "            'precision': 0,\n",
        "            'recall': 0,\n",
        "            'unpredicted_labels':[]\n",
        "        },\n",
        "        'rel_gt_instances_sup': {\n",
        "            'f1': 0,\n",
        "            'precision': 0,\n",
        "            'recall': 0\n",
        "        },\n",
        "        'rel_gt_activities_sup': {\n",
        "            'f1': [],\n",
        "            'precision': 0,\n",
        "            'recall': 0,\n",
        "            'unpredicted_labels':[]\n",
        "        },\n",
        "        'rel_instances': {\n",
        "            'f1': 0,\n",
        "            'precision': 0,\n",
        "            'recall': 0,\n",
        "        },\n",
        "        'rel_activities_from_sup_inst': {\n",
        "            'f1': [],\n",
        "            'precision': 0,\n",
        "            'recall': 0,\n",
        "            'unpredicted_labels':[]\n",
        "        },        \n",
        "        'rel_instances_collab': {\n",
        "            'f1': [],\n",
        "            'precision': [],\n",
        "            'recall': []\n",
        "        },\n",
        "        'rel_activities_from_sup_inst_collab': {\n",
        "            'f1': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'unpredicted_labels':[]\n",
        "        },\n",
        "        'rel_activities_from_sup_inst_frozen_threshold': {\n",
        "            'f1': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'unpredicted_labels':[]\n",
        "        },\n",
        "        \n",
        "    }\n",
        "\n",
        "    for xp in results:\n",
        "        for i in xp:\n",
        "            for model in i.keys():\n",
        "                scores = i[model]\n",
        "                if model in [\n",
        "                    'rel_instances_collab', 'rel_activities_from_sup_inst_collab', \n",
        "                    'rel_instances_collab_threshold', 'rel_activities_from_sup_inst_collab_threshold',\n",
        "                    'rel_activities_from_sup_inst_frozen', 'rel_activities_from_sup_inst_frozen_threshold'\n",
        "                \n",
        "                ]:\n",
        "                    for k in scores.keys():\n",
        "                        if (k in ['unpredicted_labels']) or (k in ['f1'] and 'activities' in model):\n",
        "                            summed_results[model][k].append(scores[k])\n",
        "                        else:                \n",
        "                            summed_results[model][k].append(scores[k])\n",
        "                else:\n",
        "                    for k in scores.keys():\n",
        "                        if (k in ['unpredicted_labels']) or (k in ['f1'] and 'activities' in model):\n",
        "                            summed_results[model][k].append(scores[k])\n",
        "                        else:     \n",
        "                            summed_results[model][k] = summed_results[model][k] + scores[k]\n",
        "\n",
        "    avg_results = copy.deepcopy(summed_results)\n",
        "\n",
        "    for model in avg_results.keys():\n",
        "        if model in [\n",
        "            'rel_instances_collab', 'rel_activities_from_sup_inst_collab', \n",
        "            'rel_instances_collab_threshold', 'rel_activities_from_sup_inst_collab_threshold',\n",
        "            'rel_activities_from_sup_inst_frozen', 'rel_activities_from_sup_inst_frozen_threshold'\n",
        "        ]:\n",
        "            for k in avg_results[model]:\n",
        "                if (k == 'f1' and 'activities' in model):\n",
        "                    results = []\n",
        "                    for i in range(nb_collaborative_iter):\n",
        "                        iteration_res = np.mean(np.array([avg_results[model][k][j] for j in range(i, nb_xp*nb_collaborative_iter, nb_collaborative_iter)]), axis=0)\n",
        "                        results.append(iteration_res)\n",
        "                    avg_results[model][k] = results\n",
        "                elif (k not in ['unpredicted_labels']):\n",
        "                    tmp = copy.deepcopy(avg_results[model][k])\n",
        "                    avg_results[model][k] = [0] * nb_collaborative_iter\n",
        "                    for j in range(nb_xp * nb_collaborative_iter):\n",
        "                        try:\n",
        "                            avg_results[model][k][j % nb_collaborative_iter] = avg_results[model][k][j % nb_collaborative_iter] + tmp[j]\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    avg_results[model][k] = [avg_results[model][k][i]/nb_xp for i in list(range(nb_collaborative_iter))]\n",
        "        else:\n",
        "            for k in avg_results[model]:\n",
        "                if (k == 'f1' and 'activities' in model):\n",
        "                    avg_results[model][k] = np.mean(np.array([avg_results[model][k][i] for i in range(nb_xp)]), axis=0)\n",
        "                elif (k not in ['unpredicted_labels']):\n",
        "                    avg_results[model][k] = avg_results[model][k] / nb_xp\n",
        "                    \n",
        "    return avg_results, summed_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuQnWhXAO3ol"
      },
      "source": [
        "### Metrics initializers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlSwf0TUO3LC"
      },
      "source": [
        "summed_results_camel = {\n",
        "        'ground_truth_inst_metrics': {\n",
        "            'avg_length_inst': 0,\n",
        "            'max_length_inst': 0,\n",
        "            'avg_steps_inst': 0,\n",
        "            'max_steps_inst': 0,\n",
        "            'avg_nb_users_inst': 0,\n",
        "            'max_nb_users_inst': 0\n",
        "        },\n",
        "        'ground_truth_acts_metrics': {\n",
        "            'avg_length_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            },\n",
        "            'max_length_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            },\n",
        "            'avg_nb_users_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            },\n",
        "            'max_nb_users_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            }\n",
        "        },\n",
        "        'baseline_inst_sup_metrics': {\n",
        "            'avg_length_inst': 0,\n",
        "            'max_length_inst': 0,\n",
        "            'avg_steps_inst': 0,\n",
        "            'max_steps_inst': 0,\n",
        "            'avg_nb_users_inst': 0,\n",
        "            'max_nb_users_inst': 0\n",
        "        },\n",
        "        'baseline_act_sup_metrics': {\n",
        "            'avg_length_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            },\n",
        "            'max_length_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            },\n",
        "            'avg_nb_users_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            },\n",
        "            'max_nb_users_act': {\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            }\n",
        "        },\n",
        "        'relational_acts_metrics_from_sup_inst_frozen_threshold': {\n",
        "            'avg_length_act': [{\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            } for i in range(nb_collaborative_iter)],\n",
        "            'max_length_act': [{\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            } for i in range(nb_collaborative_iter)],\n",
        "            'avg_nb_users_act': [{\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            } for i in range(nb_collaborative_iter)],\n",
        "            'max_nb_users_act': [{\n",
        "                'ask a question': 0,\n",
        "                'assign issue': 0,\n",
        "                'automated comment issue': 0,\n",
        "                'build system update': 0,\n",
        "                'close pull request': 0,\n",
        "                'close question': 0,\n",
        "                'commit changes': 0,\n",
        "                'create issue': 0,\n",
        "                'distribute situational awareness': 0,\n",
        "                'issue comment': 0,\n",
        "                'issue update': 0,\n",
        "                'open pull request': 0,\n",
        "                'provide support': 0,\n",
        "                'publicity': 0,\n",
        "                'reopen issue': 0,\n",
        "                'resolve issue': 0,\n",
        "                'update question': 0,\n",
        "                'version release planning': 0,\n",
        "                'work started issue': 0\n",
        "            } for i in range(nb_collaborative_iter)]\n",
        "        },\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHRDbynskT76"
      },
      "source": [
        "### Average unsup results and activity metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moJkumidkW6D"
      },
      "source": [
        "def get_avg_results_unsup(results, nb_xp, nb_collaborative_iter):\n",
        "    summed_results = {\n",
        "        'baseline_instances_unsup': {\n",
        "            'f1': 0,\n",
        "            'precision': 0,\n",
        "            'recall': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for xp in results:\n",
        "        for i in xp:\n",
        "            for model in i.keys():\n",
        "                scores = i[model]\n",
        "                if model in ['rel_instances_collab', 'rel_activities_from_sup_inst_collab']:\n",
        "                    for k in scores.keys():\n",
        "                        if (k == 'unpredicted_labels') or (k in ['f1'] and 'activities' in model):\n",
        "                            summed_results[model][k].append(scores[k])\n",
        "                        else:                \n",
        "                            summed_results[model][k].append(scores[k])\n",
        "                else:\n",
        "                    for k in scores.keys():\n",
        "                        if (k == 'unpredicted_labels') or (k in ['f1'] and 'activities' in model):\n",
        "                            summed_results[model][k].append(scores[k])\n",
        "                        else:     \n",
        "                            summed_results[model][k] = summed_results[model][k] + scores[k]\n",
        "\n",
        "    avg_results = copy.deepcopy(summed_results)\n",
        "\n",
        "    for model in avg_results.keys():\n",
        "        if model in ['rel_instances_collab', 'rel_activities_from_sup_inst_collab']:\n",
        "            for k in avg_results[model]:\n",
        "\n",
        "                if (k == 'f1' and 'activities' in model):\n",
        "                    results = []\n",
        "                    for i in range(nb_collaborative_iter):\n",
        "                        iteration_res = np.mean(np.array([avg_results[model][k][j] for j in range(i, nb_xp*nb_collaborative_iter, nb_collaborative_iter)]), axis=0)\n",
        "                        results.append(iteration_res)\n",
        "                    avg_results[model][k] = results\n",
        "                elif (k not in ['unpredicted_labels']):\n",
        "\n",
        "                    tmp = copy.deepcopy(avg_results[model][k])\n",
        "                    avg_results[model][k] = [0] * nb_collaborative_iter\n",
        "                    for j in range(nb_xp * nb_collaborative_iter):\n",
        "                        try:\n",
        "                            avg_results[model][k][j % nb_collaborative_iter] = avg_results[model][k][j % nb_collaborative_iter] + tmp[j]\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    avg_results[model][k] = [avg_results[model][k][i]/nb_xp for i in list(range(nb_collaborative_iter))]\n",
        "        else:\n",
        "            for k in avg_results[model]:\n",
        "                if (k == 'f1' and 'activities' in model):\n",
        "                  try:\n",
        "                    avg_results[model][k] = np.mean(np.array([avg_results[model][k][i] for i in range(nb_xp)]), axis=0)\n",
        "                  except Exception:\n",
        "                    pass\n",
        "                elif (k not in ['unpredicted_labels']):\n",
        "                    avg_results[model][k] = avg_results[model][k] / nb_xp\n",
        "                    \n",
        "    return avg_results, summed_results\n",
        "    \n",
        "\n",
        "def get_avg_user_metrics(user_metrics, nb_xp, nb_collaborative_iter, summed_results):\n",
        "    summed_results = summed_results\n",
        "    \n",
        "    for xp in user_metrics:\n",
        "        \n",
        "        # Replace NaN\n",
        "        try:\n",
        "            xp = clean_nan_dict(xp)\n",
        "        except Exception:\n",
        "            pass\n",
        "        for key in xp.keys():\n",
        "            # Instances metrics\n",
        "            if key in [\n",
        "                'ground_truth_inst_metrics', \n",
        "                'baseline_inst_sup_metrics', \n",
        "                'baseline_inst_unsup_metrics', \n",
        "                'rel_gt_inst_sup_metrics', \n",
        "                'relational_inst_metrics', \n",
        "                'relational_inst_metrics_collab',\n",
        "                'relational_inst_metrics_collab_threshold'\n",
        "            ]:\n",
        "                if key not in ['relational_inst_metrics_collab', 'relational_inst_metrics_collab_threshold']:\n",
        "                    for subkey in xp[key]:\n",
        "                        summed_results[key][subkey] = summed_results[key][subkey] + xp[key][subkey]\n",
        "                else:\n",
        "                    for i, d in enumerate(xp[key]):\n",
        "                        for c in d.keys():\n",
        "                            summed_results[key][i][c] = summed_results[key][i][c] + xp[key][i][c]\n",
        "                        \n",
        "            # Activities metrics       \n",
        "            else:\n",
        "                if key not in [\n",
        "                    'relational_acts_metrics_from_sup_inst_collab', \n",
        "                    'relational_acts_metrics_from_sup_inst_frozen',\n",
        "                    'relational_acts_metrics_from_sup_inst_collab_threshold', \n",
        "                    'relational_acts_metrics_from_sup_inst_frozen_threshold'\n",
        "                ]:\n",
        "                    try:\n",
        "                        for subkey in xp[key].keys():\n",
        "                            for s_subkey in xp[key][subkey].keys():\n",
        "                                summed_results[key][subkey][s_subkey] = summed_results[key][subkey][s_subkey] + xp[key][subkey][s_subkey]\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                else:\n",
        "                    for i, d in enumerate(xp[key]):\n",
        "                        for c in d.keys():\n",
        "                            for elt in d[c].keys():\n",
        "                                summed_results[key][c][i][elt] = summed_results[key][c][i][elt] + xp[key][i][c][elt]\n",
        "                    \n",
        "    avg_results = copy.deepcopy(summed_results)  \n",
        "    \n",
        "    for key in avg_results.keys():\n",
        "        # Instances metrics\n",
        "        if key in [\n",
        "            'ground_truth_inst_metrics', \n",
        "            'baseline_inst_sup_metrics', \n",
        "            'baseline_inst_unsup_metrics', \n",
        "            'rel_gt_inst_sup_metrics', \n",
        "            'relational_inst_metrics', \n",
        "            'relational_inst_metrics_collab',\n",
        "            'relational_inst_metrics_collab_threshold'\n",
        "        ]:\n",
        "            if key not in ['relational_inst_metrics_collab', 'relational_inst_metrics_collab_threshold']:\n",
        "                for subkey in avg_results[key]:\n",
        "                    avg_results[key][subkey] = avg_results[key][subkey] / nb_xp\n",
        "            else:\n",
        "                for i, d in enumerate(avg_results[key]):\n",
        "                    for c in d.keys(): \n",
        "                        avg_results[key][i][c] = avg_results[key][i][c] / nb_xp\n",
        "        \n",
        "        # Activities metrics\n",
        "        else:\n",
        "            if key not in [\n",
        "                'relational_acts_metrics_from_sup_inst_collab', \n",
        "                'relational_acts_metrics_from_sup_inst_frozen',\n",
        "                'relational_acts_metrics_from_sup_inst_collab_threshold', \n",
        "                'relational_acts_metrics_from_sup_inst_frozen_threshold'\n",
        "            ]:\n",
        "                for subkey in avg_results[key].keys():\n",
        "                    for s_subkey in avg_results[key][subkey].keys():\n",
        "                        avg_results[key][subkey][s_subkey] = avg_results[key][subkey][s_subkey] / nb_xp\n",
        "            else:\n",
        "                for metric in avg_results[key].keys():\n",
        "                    for i, d in enumerate(avg_results[key][metric]):\n",
        "                        for c in d.keys():\n",
        "                            avg_results[key][metric][i][c] = avg_results[key][metric][i][c] / nb_xp\n",
        "                \n",
        "        \n",
        "    return avg_results, summed_results "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYnDVLp4F290"
      },
      "source": [
        "## Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guz95kO5J_pO"
      },
      "source": [
        "if resume:\n",
        "  nb_xp = nb_xp_total\n",
        "  \n",
        "avg_results_sup, summed_results_sup = get_avg_results_sup(results_sup, nb_xp, nb_collaborative_iter)\n",
        "avg_results_unsup, summed_results_unsup = get_avg_results_unsup(results_unsup, nb_xp, nb_collaborative_iter)\n",
        "avg_metrics, summed_metrics = get_avg_user_metrics(user_metrics, nb_xp, nb_collaborative_iter, summed_results_camel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKtyOUHrJ_pR"
      },
      "source": [
        "Save results dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClrA3kcyJ_pT"
      },
      "source": [
        "with open(results_sup_save_path, 'wb') as f:\n",
        "    pickle.dump(avg_results_sup, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(results_sup_save_path, 'rb') as f:\n",
        "    final_results_sup = pickle.load(f)\n",
        "    \n",
        "with open(results_unsup_save_path, 'wb') as f:\n",
        "    pickle.dump(avg_results_unsup, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(results_unsup_save_path, 'rb') as f:\n",
        "    final_results_unsup = pickle.load(f)\n",
        "    \n",
        "with open(user_metrics_save_path, 'wb') as f:\n",
        "    pickle.dump(avg_metrics, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(user_metrics_save_path, 'rb') as f:\n",
        "    final_metrics = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5zS-81u6Gap"
      },
      "source": [
        "pprint.pprint(final_results_sup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjmpGAhC6iPa"
      },
      "source": [
        "pprint.pprint(final_results_unsup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Th0AlRJ_pX"
      },
      "source": [
        "pprint.pprint(final_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqdqp-XVJ_pY"
      },
      "source": [
        "# Encodings and information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SPhzTs1J_pZ"
      },
      "source": [
        "def get_integer_mapping(le):\n",
        "    '''\n",
        "    Return a dict mapping labels to their integer values\n",
        "    from an SKlearn LabelEncoder\n",
        "    le = a fitted SKlearn LabelEncoder\n",
        "    '''\n",
        "    res = {}\n",
        "    for cl in le.classes_:\n",
        "        res.update({cl:le.transform([cl])[0]})\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKgGAqjJJ_pa"
      },
      "source": [
        "print(\"Label encoding for activity prediction:\")\n",
        "get_integer_mapping(label_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}